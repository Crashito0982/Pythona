# -*- coding: utf-8 -*-
"""
BRITIMP - Consolidador v9 (estructura nueva de PROCESADO y PENDIENTES)
-------------------------------------------------------------------------------
Versión basada en el script robusto original (Gemini) con estos ajustes:

- Estructura de carpetas:
  * PENDIENTES = carpeta ROOT (por ejemplo 'BRITIMP'), con subcarpetas:
      ASU/, CDE/, ENC/, OVD/  -> pendientes.
  * PROCESADO/AAAA-MM-DD/AGENCIA/  -> copia del original + archivo renombrado
    con sufijo _OK o _ERROR.
  * CONSOLIDADO/AAAA-MM-DD/BRITIMP_*.csv  -> se mantiene igual.

- Validación inicial:
  * Si alguna de las carpetas de agencia (ASU, CDE, ENC, OVD) no existe o está
    vacía (sin archivos .xlsx/.xls/.pdf), se loguea el error y NO se procesa
    nada.

- Logging:
  * Se agrega un mensaje explícito cuando un parser se ejecuta pero no genera
    registros: "[SIN REGISTROS] 'archivo' no generó registros para el tipo X".
  * Se mantiene el log existente cuando se detecta "SIN MOVIMIENTOS" en el
    contenido del archivo.

- Parsers y lógica de detección:
  * Se mantiene la lógica detallada del v9 original para:
    - dispatch_tipo (por nombre + contenido),
    - detect_cliente_itau (descarta otros bancos),
    - parsers EC_EFECT_BCO, EC_EFECT_ATM, EC_BULTO_ATM, INV_BILLETES_*,
      evitando la simplificación que hacía que se salten archivos.
-------------------------------------------------------------------------------
"""

from __future__ import annotations

import os
import re
import sys
import shutil
import unicodedata
import logging
from pathlib import Path
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Tuple

import pandas as pd

try:
    from pypdf import PdfReader
except ImportError:
    PdfReader = None
    print(
        "ADVERTENCIA: La librería 'pypdf' no está instalada. "
        "El procesamiento de archivos PDF no funcionará. "
        "Instálala con: pip install pypdf"
    )

try:
    import xlrd
except ImportError:
    xlrd = None
    print(
        "ADVERTENCIA: La librería 'xlrd' no está instalada. "
        "El procesamiento de archivos .xls antiguos podría fallar. "
        "Instálala con: pip install xlrd==1.2.0"
    )


# -------------------------------------------------------------------
# CONFIGURACIÓN DE COLUMNAS DE SALIDA
# -------------------------------------------------------------------

FINAL_COLUMNS = {
    "EC_EFECT_BCO": [
        "FECHA_RECIBO",
        "SUCURSAL",
        "RECIBO",
        "BULTOS",
        "IMPORTE",
        "MONEDA",
        "ING_EGR",
        "CLASIFICACION",
        "FECHA_ARCHIVO",
        "MOTIVO_MOVIMIENTO",
        "AGENCIA",
    ],
    "EC_EFECT_ATM": [
        "FECHA_RECIBO",
        "SUCURSAL",
        "RECIBO",
        "BULTOS",
        "MONTO",
        "MONEDA",
        "ING_EGR",
        "CLASIFICACION",
        "FECHA_ARCHIVO",
        "MOTIVO_MOVIMIENTO",
        "AGENCIA",
    ],
    "INV_BILLETES_BCO": [
        "FECHA_INVENTARIO",
        "DIVISA",
        "AGENCIA",
        "AGRUPACION_EFECTIVO",
        "TIPO_VALOR",
        "DENOMINACION",
        "CALIDAD_DEPOSITO",
        "CALIDAD_CD",
        "CALIDAD_CANJE",
        "MONEDA",
        "IMPORTE_TOTAL",
    ],
    "INV_BILLETES_ATM": [
        "FECHA_INVENTARIO",
        "DIVISA",
        "AGENCIA",
        "AGRUPACION_EFECTIVO",
        "TIPO_VALOR",
        "DENOMINACION",
        "CALIDAD_DEPOSITO",
        "CALIDAD_CD",
        "CALIDAD_CANJE",
        "MONEDA",
        "IMPORTE_TOTAL",
    ],
    "EC_BULTO_ATM": [
        "FECHA_RECIBO",
        "SUCURSAL",
        "RECIBO",
        "BULTOS",
        "MONTO",
        "MONEDA",
        "ING_EGR",
        "CLASIFICACION",
        "FECHA_ARCHIVO",
        "MOTIVO_MOVIMIENTO",
        "AGENCIA",
    ],
}


def resolve_root() -> Path:
    """
    Determina el directorio raíz del proyecto.

    - Si el .py está dentro de una carpeta llamada 'BRITIMP', esa será la raíz.
    - Si existe una subcarpeta 'BRITIMP', se usa esa.
    - En otro caso, se usa el cwd.
    """
    here = Path(__file__).resolve().parent if "__file__" in globals() else Path.cwd()
    if (here / "PENDIENTES").exists() or (here.name.upper() == "BRITIMP"):
        return here
    if (here / "BRITIMP").exists():
        return here / "BRITIMP"
    return here


ROOT = resolve_root()

# NUEVA ESTRUCTURA:
# - PENDIENTES = ROOT (ej: BRITIMP), con subcarpetas ASU/CDE/ENC/OVD
# - PROCESADO = ROOT/PROCESADO
# - CONSOLIDADO = ROOT/CONSOLIDADO
PENDIENTES = ROOT
PROCESADO = ROOT / "PROCESADO"
CONSOLIDADO = ROOT / "CONSOLIDADO"

AGENCIES = ["ASU", "CDE", "ENC", "OVD"]


def ensure_dirs() -> None:
    """
    Crea solo las carpetas PROCESADO y CONSOLIDADO si no existen.
    Las carpetas de agencias bajo ROOT (ASU, CDE, ENC, OVD) se asume que
    son gestionadas externamente y se validan aparte.
    """
    for d in [PROCESADO, CONSOLIDADO]:
        d.mkdir(parents=True, exist_ok=True)


ensure_dirs()


# -------------------------------------------------------------------
# LOGGING
# -------------------------------------------------------------------

_LOGGER: Optional[logging.Logger] = None


def today_folder() -> Path:
    """Crea y devuelve la ruta a la carpeta de consolidados del día de hoy."""
    today = datetime.now().strftime("%Y-%m-%d")
    outdir = CONSOLIDADO / today
    outdir.mkdir(parents=True, exist_ok=True)
    return outdir


def setup_logger() -> logging.Logger:
    """Configura el logger para que escriba en archivo y en consola."""
    global _LOGGER
    if _LOGGER:
        return _LOGGER

    log_dir = today_folder()
    log_path = log_dir / "BRITIMP_log.txt"

    logger = logging.getLogger("BRITIMP")
    logger.setLevel(logging.INFO)
    logger.handlers = []

    fmt = logging.Formatter(
        "%(asctime)s - %(levelname)s - %(message)s", datefmt="%Y-%m-%d %H:%M:%S"
    )

    # Archivo
    fh = logging.FileHandler(log_path, encoding="utf-8", mode="a")
    fh.setFormatter(fmt)
    logger.addHandler(fh)

    # Consola
    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(fmt)
    logger.addHandler(ch)

    logger.info("=" * 10 + " INICIO DE EJECUCIÓN " + "=" * 10)
    logger.info(f"Directorio Raíz: {ROOT}")

    _LOGGER = logger
    return logger


def log_info(msg: str) -> None:
    setup_logger().info(msg)


def log_warn(msg: str) -> None:
    setup_logger().warning(msg)


def log_error(msg: str, **kwargs) -> None:
    """
    Logger de error que admite kwargs (por ejemplo, exc_info=True).
    Esto permite usar log_error(..., exc_info=True) sin romper.
    """
    setup_logger().error(msg, **kwargs)


# -------------------------------------------------------------------
# HELPERS (Funciones de Utilidad)
# -------------------------------------------------------------------

DATE_RE = re.compile(r"^\s*(\d{1,2}/\d{1,2}/\d{4})\s*$", re.IGNORECASE)


def has_string_date(row_vals: List[Any]) -> bool:
    """Verifica si alguna celda en la fila es una fecha en formato string dd/mm/yyyy."""
    for c in row_vals:
        if isinstance(c, str) and DATE_RE.match(c.strip()):
            return True
        if isinstance(c, datetime):
            return True
    return False


def excel_serial_to_ddmmyyyy(val: float) -> Optional[str]:
    """Convierte fecha serial de Excel a 'dd/mm/yyyy'."""
    try:
        if isinstance(val, (int, float)) and not pd.isna(val):
            base = datetime(1899, 12, 30)
            return (base + timedelta(days=float(val))).date().strftime("%d/%m/%Y")
    except Exception:
        return None
    return None


def to_ddmmyyyy(val: Any) -> Optional[str]:
    """Convierte varios formatos de fecha a 'dd/mm/yyyy'."""
    if isinstance(val, datetime):
        return val.strftime("%d/%m/%Y")
    if isinstance(val, (int, float)) and not pd.isna(val):
        d = excel_serial_to_ddmmyyyy(val)
        if d:
            return d
    if isinstance(val, str):
        s = val.strip()
        if DATE_RE.match(s):
            return s
        for fmt in ("%Y-%m-%d", "%d-%m-%Y", "%d/%m/%y"):
            try:
                return datetime.strptime(s, fmt).strftime("%d/%m/%Y")
            except ValueError:
                pass
    return None


NUM_SANITIZER = re.compile(r"[^\d,.\-()\u00A0 ]")


def parse_numeric(val: Any) -> Optional[float]:
    """Parsea un valor a número, manejando formatos españoles y negativos con ()."""
    if pd.isna(val):
        return None
    if isinstance(val, (int, float)):
        return float(val)
    if isinstance(val, str):
        s = NUM_SANITIZER.sub("", val).replace("\u00A0", " ").strip()
        if not s:
            return None
        neg = s.startswith("(") and s.endswith(")")
        if neg:
            s = s[1:-1].strip()
        s = s.replace(" ", "")
        if "," in s and "." in s:
            if s.rfind(".") < s.rfind(","):
                s = s.replace(".", "")
            s = s.replace(",", ".")
        elif "," in s:
            parts = s.split(",")
            if len(parts) >= 2 and len(parts[-1]) <= 2:
                s = s.replace(",", ".")
            else:
                s = s.replace(",", "")
        try:
            x = float(s)
            return -x if neg else x
        except (ValueError, TypeError):
            return None
    return None


def clean_digits(val: Any) -> str:
    """Extrae solo los dígitos de un valor."""
    return "" if pd.isna(val) else re.sub(r"\D", "", str(val))


def unir_letras_separadas(texto: str) -> str:
    """Une letras separadas por espacios, ej: 'B I L L E T E S' -> 'BILLETES'."""
    if not texto:
        return texto
    pattern = r"(?:\b[A-ZÁÉÍÓÚÑ](?:\s+[A-ZÁÉÍÓÚÑ])+\b)"
    return re.sub(pattern, lambda m: m.group(0).replace(" ", ""), texto)


def remove_accents(s: str) -> str:
    """Quita acentos y diacríticos de un texto."""
    return "".join(ch for ch in unicodedata.normalize("NFD", s) if not unicodedata.combining(ch))


def name_matches(fname: str, required_groups: List[List[str]]) -> bool:
    """Verifica si un nombre de archivo contiene tokens requeridos."""
    target = remove_accents(fname).upper()
    return all(
        any(remove_accents(opt).upper() in target for opt in group)
        for group in required_groups
    )


# -------------------------------------------------------------------
# NORMALIZACIÓN DE AGENCIA / DIVISA
# -------------------------------------------------------------------

AGENCIA_PATTERNS: Dict[str, List[str]] = {
    "ASU": [r"\bCASA\s+MATRIZ\b", r"\bASUNCION\b", r"\bASUNCIÓN\b", r"\bASU\b"],
    "CDE": [r"\bCIUDAD\s+DEL\s+ESTE\b", r"\bCDE\b"],
    "ENC": [r"\bENCARNACION\b", r"\bENCARNACIÓN\b", r"\bENC\b"],
    "OVD": [
        r"\bCNEL\.?\s+OVIEDO\b",
        r"\bCORONEL\s+OVIEDO\b",
        r"\bOVIEDO\b",
        r"\bOVD\b",
    ],
}


def normalize_agencia_to_cod(value: Any) -> str:
    """Normaliza el nombre de una agencia a su código de 3 letras."""
    if not value:
        return ""
    u = remove_accents(str(value).strip().upper())
    for cod, patterns in AGENCIA_PATTERNS.items():
        if any(re.search(pat, u) for pat in patterns):
            return cod
    return ""


def normalize_divisa_to_iso(value: Any) -> str:
    """Normaliza variantes de moneda a 'PYG' o 'USD'."""
    if not value:
        return ""
    u = remove_accents(str(value).strip().upper()).replace("₲", "GS").replace("US$", "USD")
    canon = re.sub(r"[^A-Z0-9]", "", u)
    if canon.startswith("GUAR") or canon in {"PYG", "GS", "GUARANI", "GUARANIES"}:
        return "PYG"
    if canon.startswith("DOL") or "USD" in canon or canon in {"US", "USS"}:
        return "USD"
    return ""


# -------------------------------------------------------------------
# LECTURA Y DETECCIÓN
# -------------------------------------------------------------------

NEGATIVE_BANKS = [
    "CONTINENTAL",
    "BBVA",
    "GNB",
    "REGIONAL",
    "BASA",
    "VISION",
    "ATLAS",
    "SUDAMERIS",
    "FAMILIAR",
    "ITAPUA",
    "AMAMBAY",
]


def read_excel_any_version(path: Path) -> Optional[pd.ExcelFile]:
    """Lee un archivo Excel, intentando con varios motores si es necesario."""
    try:
        return pd.ExcelFile(path)
    except Exception:
        try:
            engine = (
                "openpyxl"
                if path.suffix.lower() == ".xlsx"
                else "xlrd"
                if xlrd
                else None
            )
            if engine:
                return pd.ExcelFile(path, engine=engine)
        except Exception as e:
            log_warn(
                f"No se pudo leer el archivo Excel '{path.name}' con ningún motor. Error: {e}"
            )
    return None


def file_text_preview(path: Path, max_rows_excel: int = 40) -> str:
    """Extrae un texto de previsualización de un archivo (PDF o Excel)."""
    ext = path.suffix.lower()
    if ext in (".xlsx", ".xls"):
        xl = read_excel_any_version(path)
        if not xl:
            return ""
        parts: List[str] = []
        for sh_name in xl.sheet_names:
            df = pd.read_excel(xl, sheet_name=sh_name, header=None, nrows=max_rows_excel)
            if not df.empty:
                parts.extend(
                    df.fillna("")
                    .astype(str)
                    .agg(" ".join, axis=1)
                    .tolist()
                )
        return "\n".join(parts)
    if ext == ".pdf" and PdfReader:
        try:
            return "".join(page.extract_text() or "" for page in PdfReader(path).pages)
        except Exception:
            return ""
    return ""


def detect_cliente_itau(path: Path, tipo_documento: Optional[str]) -> bool:
    """
    Verifica si el documento es de Itaú. Primero busca explícitamente otros bancos
    con el formato "CLIENTE: [BANCO]" para evitar falsos positivos.
    """
    txt = unir_letras_separadas(file_text_preview(path)).upper()

    # Descarte explícito de otros bancos
    for bank in NEGATIVE_BANKS:
        pattern = r"CLIENTE:\s*" + re.escape(bank)
        if re.search(pattern, txt):
            log_info(f"[SKIP] {path.name} -> Otro banco detectado explícitamente: '{bank}'")
            return False

    # Si no se descarta, se procede con la lógica de aceptación para Itaú
    fname_upper = remove_accents(path.name).upper()
    if "ITAU" in txt or "ITAU" in fname_upper:
        return True

    # Para Estados de Cuenta (EC), si no se menciona "ITAU" explícitamente, se descarta por seguridad.
    if tipo_documento and "EC_" in tipo_documento:
        if name_matches(path.name, [["EC"], ["ATM", "BCO", "BANCO", "BULTO"], ["EFECTIVO", "BILLETE"]]):
            log_info(f"[SKIP] {path.name} -> Es Estado de Cuenta pero no menciona 'ITAU'.")
            return False

    # Para Inventarios (INV), somos más permisivos. Si el nombre coincide con el patrón, se acepta.
    if tipo_documento and "INV_" in tipo_documento:
        if name_matches(path.name, [["INV"], ["BILLETE"], ["ATM", "BCO", "BANCO", "DOLAR"]]):
            return True

    log_info(f"[SKIP] {path.name} -> No se pudo confirmar que sea de ITAU.")
    return False


def detect_sin_movimientos(path: Path) -> bool:
    """Detecta si el archivo contiene la frase 'SIN MOVIMIENTOS' con espacios flexibles."""
    txt = unir_letras_separadas(file_text_preview(path)).upper()
    return bool(re.search(r"SIN\s+MOVIMIENTOS?", txt))


def parse_agencia_from_text(text: str) -> str:
    """Extrae la agencia desde el texto (buscando 'SUC: ...')."""
    m = re.search(r"SUC:\s*(.+?)\s*(?:[\)\]]|$)", text, flags=re.IGNORECASE)
    if m:
        raw_agencia = m.group(1).strip()
        cod = normalize_agencia_to_cod(raw_agencia)
        return cod or raw_agencia
    return ""


# -------------------------------------------------------------------
# DISPATCHER (Selector de Tipo)
# -------------------------------------------------------------------

def dispatch_tipo(fname: str, text_content: str = "") -> Optional[str]:
    """Determina el tipo de documento basado en el nombre del archivo y su contenido."""
    up = remove_accents(fname).upper()

    # Reglas prioritarias para PDF de inventario por nombre
    if up.endswith(".PDF"):
        if name_matches(up, [["INV"], ["ATM"]]):
            return "INV_BILLETES_ATM"
        if name_matches(up, [["INV"], ["BANCO"]]):
            return "INV_BILLETES_BCO"
        if name_matches(up, [["INV"], ["DOLAR"]]):
            return "INV_BILLETES_BCO"

    # Reglas generales
    if name_matches(up, [["EC"], ["BULTO"], ["ATM"]]):
        return "EC_BULTO_ATM"
    if name_matches(up, [["EC"], ["EFECT"], ["ATM"]]):
        return "EC_EFECT_ATM"
    if name_matches(up, [["EC"], ["EFECT"], ["BCO", "BANCO"]]):
        return "EC_EFECT_BCO"
    if name_matches(up, [["INV"], ["BILLETE"], ["ATM"]]):
        return "INV_BILLETES_ATM"
    if name_matches(up, [["INV"], ["BILLETE"], ["BCO", "BANCO", "DOLAR"]]):
        return "INV_BILLETES_BCO"

    # Fallback para PDFs con nombre genérico, usando contenido
    if "INV" in up:
        up_text = text_content.upper()
        if "INVENTARIO DE BILLETES DE ATM" in up_text:
            return "INV_BILLETES_ATM"
        if "INVENTARIO DE BILLETES DE BANCO" in up_text:
            return "INV_BILLETES_BCO"
        return "INV_BILLETES_UNKNOWN"

    return None


# -------------------------------------------------------------------
# PARSERS (Extractores de Datos)
# -------------------------------------------------------------------

def parse_ec_bultos_atm_xlsx(path: Path, DEBUG: bool = False) -> pd.DataFrame:
    """Parser para 'Estado de Cuenta Bultos ATM' (.xls y .xlsx)."""
    xl = read_excel_any_version(path)
    if not xl:
        return pd.DataFrame()
    registros: List[dict] = []

    for sheet_name in xl.sheet_names:
        df_raw = pd.read_excel(xl, sheet_name=sheet_name, header=None)
        full_text = "\n".join(df_raw.fillna("").astype(str).agg(" ".join, axis=1))
        agencia = parse_agencia_from_text(full_text)
        m_fecha = re.search(
            r"ESTADO\s+DE\s+CUENTA.*DEL\s*:\s*(\d{2}/\d{2}/\d{4})",
            full_text,
            re.IGNORECASE,
        )
        fecha_archivo = m_fecha.group(1) if m_fecha else None

        section, motivo, started = None, None, False
        for _, row_series in df_raw.iterrows():
            row = row_series.tolist()
            txt = " ".join(map(str, filter(pd.notna, row))).upper()

            if "SALDO ANTERIOR" in txt:
                section, motivo, started = None, None, False
                continue
            if "INGRESOS" in txt and "EGRESOS" not in txt:
                section, motivo, started = "INGRESOS", None, True
                continue
            if "EGRESOS" in txt:
                section, motivo, started = "EGRESOS", None, True
                continue
            if "INFORME DE PROCESOS" in txt:
                break
            if not started:
                continue
            if txt.startswith("TOTAL") and not has_string_date(row):
                motivo = None
                continue

            nonempty = [x for x in row if pd.notna(x) and str(x).strip()]
            if len(nonempty) == 1 and not to_ddmmyyyy(nonempty[0]):
                motivo = str(nonempty[0]).strip()
                continue

            fecha, sucursal, recibo, idx = None, "", "", 0

            # Fecha
            while idx < len(row):
                f = to_ddmmyyyy(row[idx])
                if f:
                    fecha = f
                    idx += 1
                    break
                idx += 1
            if not fecha:
                continue

            # Sucursal
            while idx < len(row):
                sval = str(row[idx]).strip()
                if sval:
                    sucursal = sval
                    idx += 1
                    break
                idx += 1
            if not sucursal:
                continue

            # Recibo
            while idx < len(row):
                digits = clean_digits(row[idx])
                if len(digits) >= 5:
                    recibo = digits
                    idx += 1
                    break
                idx += 1
            if not recibo:
                continue

            # Montos
            nums = [parse_numeric(c) for c in row[idx:] if pd.notna(c)]
            nums = [n for n in nums if n is not None]
            while len(nums) < 4:
                nums.append(0.0)
            bgs, mgs, busd, musd = nums[0], nums[1], nums[2], nums[3]

            def add_row(moneda: str, bultos: Optional[float], monto: Optional[float]) -> None:
                registros.append(
                    {
                        "FECHA_OPERACION": fecha,
                        "SUCURSAL": sucursal,
                        "RECIBO": recibo,
                        "BULTOS": int(bultos) if bultos else None,
                        "MONTO": float(monto or 0.0),
                        "MONEDA": moneda,
                        "ING_EGR": "IN" if section == "INGRESOS" else "OUT",
                        "CLASIFICACION": "ATM",
                        "FECHA_ARCHIVO": fecha_archivo,
                        "MOTIVO_MOVIMIENTO": (motivo or section),
                        "AGENCIA": agencia,
                        "ARCHIVO_ORIGEN": path.name,
                    }
                )

            if (bgs or mgs):
                add_row("PYG", bgs, mgs)
            if (busd or musd):
                add_row("USD", busd, musd)

    return pd.DataFrame(registros)


def parse_ec_efect_bco_xlsx(path: Path, DEBUG: bool = False) -> pd.DataFrame:
    """Parser específico para 'Estado de Cuenta Efectivo Banco' (.xls y .xlsx)."""
    xl = read_excel_any_version(path)
    if not xl:
        return pd.DataFrame()

    all_dfs: List[pd.DataFrame] = []

    for sheet_name in xl.sheet_names:
        df_raw = pd.read_excel(xl, sheet_name=sheet_name, header=None)

        full_text = " ".join(df_raw.astype(str).agg(" ".join, axis=1))
        moneda = "USD" if "MONEDA: DOLAR" in full_text.upper() else "PYG"
        agencia = parse_agencia_from_text(full_text)
        m_fecha = re.search(
            r"BANCO\s+DEL:\s*(\d{2}/\d{2}/\d{4})",
            full_text,
            flags=re.IGNORECASE,
        )
        fecha_archivo = m_fecha.group(1) if m_fecha else None
        extra_cliente = (
            " (DOCUMENTA/ITAU)" if "CLIENTE: DOCUMENTA /ITAU" in full_text.upper() else ""
        )

        registros: List[Dict[str, Any]] = []
        current_section: Optional[str] = None
        current_motivo: Optional[str] = None

        for _, row_series in df_raw.iterrows():
            row = row_series.tolist()
            txt = " ".join(
                map(str, filter(lambda x: pd.notna(x) and str(x).strip(), row))
            ).upper()

            if "INFORME DE PROCESOS" in txt:
                break
            if "INGRESOS" in txt and "EGRESOS" not in txt:
                current_section, current_motivo = "INGRESOS", None
                continue
            if "EGRESOS" in txt:
                current_section, current_motivo = "EGRESOS", None
                continue
            if not current_section:
                continue
            if txt.startswith("TOTAL") and not has_string_date(row):
                current_motivo = None
                continue

            nonempty = [x for x in row if pd.notna(x) and str(x).strip()]
            if len(nonempty) == 1 and not to_ddmmyyyy(nonempty[0]):
                current_motivo = str(nonempty[0]).strip()
                continue

            fecha, sucursal, recibo, bultos, idx = None, "", "", None, 0

            # Fecha
            while idx < len(row):
                f = to_ddmmyyyy(row[idx])
                if f:
                    fecha = f
                    idx += 1
                    break
                idx += 1
            if not fecha:
                continue

            # Sucursal
            while idx < len(row):
                sval = str(row[idx]).strip()
                if sval:
                    sucursal = sval
                    idx += 1
                    break
                idx += 1
            if not sucursal:
                continue

            # Recibo
            while idx < len(row):
                digits = clean_digits(row[idx])
                if len(digits) >= 5:
                    recibo = digits
                    idx += 1
                    break
                idx += 1

            # Bultos (opcional)
            if idx < len(row):
                b = parse_numeric(row[idx])
                if b is not None and b == int(b) and 0 <= b < 1000:
                    bultos = int(b)
                    idx += 1

            importe_candidates = [
                parse_numeric(c) for c in row[idx:] if parse_numeric(c) is not None
            ]
            if not importe_candidates:
                continue

            registros.append(
                {
                    "FECHA_OPERACION": fecha,
                    "SUCURSAL": sucursal,
                    "RECIBO": recibo,
                    "BULTOS": bultos,
                    "IMPORTE": max(importe_candidates),
                    "MONEDA": moneda,
                    "ING_EGR": "IN" if current_section == "INGRESOS" else "OUT",
                    "CLASIFICACION": "BCO",
                    "FECHA_ARCHIVO": fecha_archivo,
                    "MOTIVO_MOVIMIENTO": (current_motivo or current_section) + extra_cliente,
                    "AGENCIA": agencia,
                    "ARCHIVO_ORIGEN": path.name,
                }
            )

        if registros:
            all_dfs.append(pd.DataFrame(registros))

    if not all_dfs:
        return pd.DataFrame()
    return pd.concat(all_dfs, ignore_index=True)


def parse_inv_billetes_pdf_bco(path: Path, DEBUG: bool = False) -> pd.DataFrame:
    """Parser para PDF de Inventario de Banco."""
    if not PdfReader:
        return pd.DataFrame()
    try:
        texto = "".join(p.extract_text() or "" for p in PdfReader(path).pages)
        texto = unir_letras_separadas(texto)
    except Exception as e:
        log_warn(f"No se pudo leer el PDF '{path.name}'. Error: {e}")
        return pd.DataFrame()

    m_fecha = re.search(
        r"SALDO DE INVENTARIO DE BILLETES AL:\s*(\d{2}-\d{2}-\d{4})",
        texto,
        re.IGNORECASE,
    )
    fecha_inventario = m_fecha.group(1).replace("-", "/") if m_fecha else None

    divisa = "USD" if "DOLAR" in texto.upper() or "DOLAR" in path.name.upper() else "PYG"
    agencia_raw = (
        re.search(r"SUC:\s*(.+)", texto, re.IGNORECASE) or [None, ""]
    )[1].strip()
    agencia = normalize_agencia_to_cod(agencia_raw) or agencia_raw

    cliente_documenta = "DOCUMENTA /ITAU" in texto.upper()
    datos: List[List[Any]] = []
    agrupacion: Optional[str] = None
    tipo_valor: Optional[str] = None
    RE_NUM = re.compile(r"\b\d{1,3}(?:\.\d{3})*\b")

    for linea in texto.split("\n"):
        linea = linea.strip()
        if not linea or "SUB-TOTAL" in linea.upper() or "TOTAL" in linea.upper():
            continue

        m_agrup = re.search(
            r"^(TESORO|PICOS|FAJOS)\s+EFECTIVO",
            linea,
            re.IGNORECASE,
        )
        if m_agrup:
            agrupacion = (
                f"{m_agrup.group(0)} (DOCUMENTA/ITAU)"
                if cliente_documenta
                else m_agrup.group(0)
            )
            continue

        if re.search(r"^(BILLETES|MONEDAS)", linea, re.IGNORECASE):
            tipo_valor = linea
            continue

        numeros = [n.replace(".", "") for n in RE_NUM.findall(linea)]
        if len(numeros) == 6 and agrupacion and tipo_valor:
            datos.append(
                [fecha_inventario, divisa, agencia, agrupacion, tipo_valor, *numeros]
            )

    cols = [
        "FECHA_INVENTARIO",
        "DIVISA",
        "AGENCIA",
        "AGRUPACION_EFECTIVO",
        "TIPO_VALOR",
        "DENOMINACION",
        "CALIDAD_DEPOSITO",
        "CALIDAD_CD",
        "CALIDAD_CANJE",
        "MONEDA",
        "IMPORTE_TOTAL",
    ]
    df = pd.DataFrame(datos, columns=cols)
    if not df.empty:
        df["ARCHIVO_ORIGEN"] = path.name
    return df


def parse_inv_billetes_pdf_atm(path: Path, DEBUG: bool = False) -> pd.DataFrame:
    """Parser para PDF de Inventario de ATM."""
    if not PdfReader:
        return pd.DataFrame()
    try:
        texto = "".join(p.extract_text() or "" for p in PdfReader(path).pages)
        texto = unir_letras_separadas(texto)
    except Exception as e:
        log_warn(f"No se pudo leer el PDF '{path.name}'. Error: {e}")
        return pd.DataFrame()

    m_fecha = re.search(
        r"SALDO DE INVENTARIO DE BILLETES AL:\s*(\d{2}-\d{2}-\d{4})",
        texto,
        re.IGNORECASE,
    )
    fecha_inventario = m_fecha.group(1).replace("-", "/") if m_fecha else None

    agencia_raw = (
        re.search(r"SUC:\s*([A-ZÁÉÍÓÚÑ ]+)", texto, re.IGNORECASE) or [None, ""]
    )[1].strip()
    agencia = normalize_agencia_to_cod(agencia_raw) or agencia_raw

    datos: List[List[Any]] = []
    agrupacion: Optional[str] = None
    tipo_valor: Optional[str] = None
    divisa = "PYG"
    RE_NUM = re.compile(r"\d{1,3}(?:\.\d{3})*")
    RE_ATM_GRUPO = re.compile(
        r"^\s*(TESORO|TESOSO|PICOS|FAJOS)\b.*?\s+ATM\b",
        re.IGNORECASE,
    )

    for linea in texto.split("\n"):
        linea = linea.strip()
        if not linea or "SUB-TOTAL" in linea.upper() or "TOTAL" in linea.upper():
            continue

        if RE_ATM_GRUPO.match(linea):
            agrupacion = linea
            divisa = "USD" if "USD" in linea.upper() or "MDA" in linea.upper() else "PYG"
            continue

        if re.search(r"^\s*(BILLETES|MONEDAS)\b", linea, re.IGNORECASE):
            tipo_valor = linea
            continue

        numeros = [n.replace(".", "") for n in RE_NUM.findall(linea)]
        if len(numeros) == 6 and agrupacion and tipo_valor:
            datos.append(
                [fecha_inventario, divisa, agencia, agrupacion, tipo_valor, *numeros]
            )

    cols = [
        "FECHA_INVENTARIO",
        "DIVISA",
        "AGENCIA",
        "AGRUPACION_EFECTIVO",
        "TIPO_VALOR",
        "DENOMINACION",
        "CALIDAD_DEPOSITO",
        "CALIDAD_CD",
        "CALIDAD_CANJE",
        "MONEDA",
        "IMPORTE_TOTAL",
    ]
    df = pd.DataFrame(datos, columns=cols)
    if not df.empty:
        df["ARCHIVO_ORIGEN"] = path.name
    return df


def parse_inv_billetes_xlsx(path: Path, DEBUG: bool = False) -> pd.DataFrame:
    """Parser para Inventarios de Billetes en Excel (BCO y ATM)."""
    xl = read_excel_any_version(path)
    if not xl:
        return pd.DataFrame()
    registros: List[Dict[str, Any]] = []

    for sheet_name in xl.sheet_names:
        df_raw = pd.read_excel(xl, sheet_name=sheet_name, header=None)
        head_text = "\n".join(
            df_raw.head(15).fillna("").astype(str).agg(" ".join, axis=1)
        )

        agencia_txt = parse_agencia_from_text(head_text)
        agencia_cod = normalize_agencia_to_cod(agencia_txt)
        agencia_out = agencia_cod or agencia_txt

        m_fecha = re.search(
            r"INVENTARIO\s+DE\s+BILLETES\s+DE\s+(?:ATM|BANCO)\s+AL:\s*(\d{1,2}/\d{1,2}/\d{4})",
            head_text,
            re.IGNORECASE,
        )
        fecha_inv = m_fecha.group(1) if m_fecha else None

        for _, row_series in df_raw.iterrows():
            row = row_series.tolist()
            iso_div = normalize_divisa_to_iso(row[0])
            if iso_div not in {"PYG", "USD"}:
                continue

            agrup = str(row[1] or "").strip().rstrip(".")
            tipo = unir_letras_separadas(str(row[2] or "").strip())
            denom = parse_numeric(row[3])
            if denom is None:
                continue

            imp = parse_numeric(row[8] if len(row) > 8 else None)
            if imp is None:
                continue

            registros.append(
                {
                    "FECHA_INVENTARIO": fecha_inv,
                    "DIVISA": iso_div,
                    "AGENCIA": agencia_out,
                    "AGRUPACION_EFECTIVO": agrup,
                    "TIPO_VALOR": tipo,
                    "DENOMINACION": int(denom),
                    "CALIDAD_DEPOSITO": int(parse_numeric(row[4] or 0)),
                    "CALIDAD_CD": int(parse_numeric(row[5] or 0)),
                    "CALIDAD_CANJE": int(parse_numeric(row[6] or 0)),
                    "MONEDA": int(parse_numeric(row[7] or 0)),
                    "IMPORTE_TOTAL": float(imp),
                    "ARCHIVO_ORIGEN": path.name,
                }
            )
    return pd.DataFrame(registros)


def parse_ec_efect_atm_xlsx(path: Path, DEBUG: bool = False) -> pd.DataFrame:
    """Parser específico y robusto para 'Estado de Cuenta Efectivo ATM' (.xls y .xlsx)."""
    xl = read_excel_any_version(path)
    if not xl:
        return pd.DataFrame()
    registros: List[dict] = []

    for sheet_name in xl.sheet_names:
        df_raw = pd.read_excel(xl, sheet_name=sheet_name, header=None)
        full_text = "\n".join(df_raw.fillna("").astype(str).agg(" ".join, axis=1))
        agencia = parse_agencia_from_text(full_text)

        m_fecha = re.search(
            r"ESTADO\s+DE\s+CUENTA\s+(?:DE\s+EFECTIVO\s+)?DE\s+ATM\s+DEL:\s*(\d{1,2}/\d{1,2}/\d{4})",
            full_text,
            re.IGNORECASE,
        )
        fecha_archivo = m_fecha.group(1) if m_fecha else None

        section, motivo, started = None, None, False
        for _, row_series in df_raw.iterrows():
            row = row_series.tolist()
            txt_upper = " ".join(map(str, filter(pd.notna, row))).upper().strip()

            if "INFORME DE PROCESOS" in txt_upper:
                break
            if txt_upper == "INGRESOS":
                section, motivo, started = "INGRESOS", None, True
                continue
            if txt_upper == "EGRESOS":
                section, motivo, started = "EGRESOS", None, True
                continue
            if not started:
                continue
            if txt_upper.startswith("TOTAL") and not has_string_date(row):
                motivo, section = None, None
                continue

            nonempty = [x for x in row if pd.notna(x) and str(x).strip()]
            if len(nonempty) == 1 and not to_ddmmyyyy(nonempty[0]):
                motivo = str(nonempty[0]).strip()
                continue

            idx, fecha, sucursal, recibo, bultos = 0, None, "", "", None

            # Fecha
            while idx < len(row):
                f = to_ddmmyyyy(row[idx])
                if f:
                    fecha = f
                    idx += 1
                    break
                idx += 1
            if not fecha:
                continue

            # Sucursal
            while idx < len(row):
                sval = str(row[idx]).strip()
                if sval:
                    sucursal = sval
                    idx += 1
                    break
                idx += 1
            if not sucursal:
                continue

            # Recibo
            while idx < len(row):
                digits = clean_digits(row[idx])
                if len(digits) >= 5:
                    recibo = digits
                    idx += 1
                    break
                idx += 1
            if not recibo:
                continue

            # Bultos (opcional)
            if idx < len(row):
                b = parse_numeric(row[idx])
                if b is not None and 0 <= b < 1000 and float(b).is_integer():
                    bultos, idx = int(b), idx + 1

            # Montos (cola numérica)
            nums_tail = [parse_numeric(c) for c in row[idx:] if pd.notna(c)]
            nums_tail = [n for n in nums_tail if n is not None]

            monto_gs, monto_usd = 0.0, 0.0
            if len(nums_tail) >= 2:
                monto_gs, monto_usd = nums_tail[-2], nums_tail[-1]
            elif len(nums_tail) == 1:
                monto_gs = nums_tail[0]

            def add_row(moneda: str, monto: float) -> None:
                registros.append(
                    {
                        "FECHA_OPERACION": fecha,
                        "SUCURSAL": sucursal,
                        "RECIBO": recibo,
                        "BULTOS": bultos,
                        "MONTO": monto,
                        "MONEDA": moneda,
                        "ING_EGR": "IN" if section == "INGRESOS" else "OUT",
                        "CLASIFICACION": "ATM",
                        "FECHA_ARCHIVO": fecha_archivo,
                        "MOTIVO_MOVIMIENTO": (motivo or section),
                        "AGENCIA": agencia,
                        "ARCHIVO_ORIGEN": path.name,
                    }
                )

            if monto_gs:
                add_row("PYG", monto_gs)
            if monto_usd:
                add_row("USD", monto_usd)

    return pd.DataFrame(registros)


# -------------------------------------------------------------------
# MAPEOS DE ARCHIVOS DE SALIDA
# -------------------------------------------------------------------

OUTPUT_FILES = {
    "EC_EFECT_BCO": "BRITIMP_EFECTBANCO.csv",
    "EC_EFECT_ATM": "BRITIMP_EFECTATM.csv",
    "INV_BILLETES_BCO": "BRITIMP_INVENTARIO_BANCO.csv",
    "INV_BILLETES_ATM": "BRITIMP_INVENTARIO_ATM.csv",
    "EC_BULTO_ATM": "BRITIMP_BULTOS_ATM.csv",
}


# -------------------------------------------------------------------
# POST-PROCESAMIENTO
# -------------------------------------------------------------------

def post_process_dataframes(data_dict: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:
    """Aplica limpiezas finales a los dataframes: renombra, formatea y elimina columnas."""
    processed_dict: Dict[str, pd.DataFrame] = {}
    for tipo, df in data_dict.items():
        if df.empty:
            continue

        df_copy = df.copy()

        # Eliminar ARCHIVO_ORIGEN
        if "ARCHIVO_ORIGEN" in df_copy.columns:
            df_copy = df_copy.drop(columns=["ARCHIVO_ORIGEN"])

        # Estados de Cuenta (EC)
        if tipo.startswith("EC_"):
            if "FECHA_OPERACION" in df_copy.columns:
                df_copy = df_copy.rename(columns={"FECHA_OPERACION": "FECHA_RECIBO"})

            for col in ["MONTO", "IMPORTE", "BULTOS"]:
                if col in df_copy.columns:
                    df_copy[col] = (
                        pd.to_numeric(df_copy[col], errors="coerce").astype("Int64")
                    )

        # Inventarios (INV)
        if tipo.startswith("INV_"):
            if "CJE_DEP" in df_copy.columns:
                if "CALIDAD_CD" not in df_copy:
                    df_copy["CALIDAD_CD"] = None
                df_copy["CALIDAD_CD"] = df_copy["CALIDAD_CD"].fillna(df_copy["CJE_DEP"])
                df_copy = df_copy.drop(columns=["CJE_DEP"])

            for col in [
                "DENOMINACION",
                "CALIDAD_DEPOSITO",
                "CALIDAD_CD",
                "CALIDAD_CANJE",
                "MONEDA",
                "IMPORTE_TOTAL",
            ]:
                if col in df_copy.columns:
                    df_copy[col] = (
                        pd.to_numeric(df_copy[col], errors="coerce").astype("Int64")
                    )

        # Orden final de columnas
        if tipo in FINAL_COLUMNS:
            final_cols = [c for c in FINAL_COLUMNS[tipo] if c in df_copy.columns]
            processed_dict[tipo] = df_copy[final_cols]
        else:
            processed_dict[tipo] = df_copy

    return processed_dict


def write_all_consolidated(all_data: Dict[str, pd.DataFrame]) -> None:
    """Escribe todos los DataFrames acumulados a sus respectivos archivos CSV, sobrescribiendo."""
    outdir = today_folder()
    log_info("--- INICIANDO ESCRITURA DE ARCHIVOS CONSOLIDADOS ---")
    for tipo, df in all_data.items():
        if tipo not in OUTPUT_FILES or df.empty:
            continue
        outpath = outdir / OUTPUT_FILES[tipo]
        try:
            df.to_csv(outpath, index=False, encoding="utf-8-sig", sep=";")
            log_info(
                f"[CSV ESCRITO] {len(df)} registros guardados en '{outpath.name}'"
            )
        except Exception as e:
            log_error(
                f"[ERROR DE ESCRITURA] No se pudo guardar '{outpath.name}'. Detalles: {e}"
            )


# -------------------------------------------------------------------
# PROCESO DE UN ARCHIVO
# -------------------------------------------------------------------

def process_file(
    path: Path,
    parent_agency_hint: Optional[str] = None,
    DEBUG: bool = False,
) -> Tuple[pd.DataFrame, Optional[str], str, bool]:
    """
    Procesa un único archivo, desde la detección hasta el parsing.

    Retorna:
      - DataFrame con registros.
      - tipo lógico (clave de OUTPUT_FILES).
      - agencia detectada (o hint).
      - bool procesado_ok (True si era de Itaú y el parser no explotó).
    """
    log_info(
        f"[ANALIZANDO] '{path.name}' en carpeta '{parent_agency_hint or 'RAIZ'}'"
    )

    preview = file_text_preview(path)
    tipo = dispatch_tipo(path.name, preview)

    # Verificación de cliente
    if not detect_cliente_itau(path, tipo):
        # No es Itaú ⇒ no se procesa, marcamos como fallo lógico
        return pd.DataFrame(), tipo, (parent_agency_hint or ""), False

    # SIN MOVIMIENTOS
    if detect_sin_movimientos(path):
        agencia = parse_agencia_from_text(preview) or parent_agency_hint or "N/A"
        log_info(
            f"[SKIP] '{path.name}' -> Archivo SIN MOVIMIENTOS (agencia: {agencia})"
        )
        return pd.DataFrame(), tipo, agencia, True

    # Refinar tipo UNKNOWN
    if tipo == "INV_BILLETES_UNKNOWN":
        log_info("-> Tipo 'UNKNOWN' necesita ser refinado por contenido...")
        if "ATM" in preview.upper():
            tipo = "INV_BILLETES_ATM"
        else:
            tipo = "INV_BILLETES_BCO"
        log_info(f"-> Tipo refinado a: {tipo}")

    agencia_final = parse_agencia_from_text(preview) or parent_agency_hint or ""
    log_info(
        f"-> Tipo detectado: {tipo or 'Desconocido'}, Agencia: {agencia_final or 'No detectada'}"
    )

    df = pd.DataFrame()
    parser_used = "Ninguno"
    ext = path.suffix.lower()

    try:
        if tipo == "EC_BULTO_ATM" and ext in (".xlsx", ".xls"):
            parser_used = "parse_ec_bultos_atm_xlsx"
            df = parse_ec_bultos_atm_xlsx(path, DEBUG)
        elif tipo == "EC_EFECT_ATM" and ext in (".xlsx", ".xls"):
            parser_used = "parse_ec_efect_atm_xlsx"
            df = parse_ec_efect_atm_xlsx(path, DEBUG)
        elif tipo == "EC_EFECT_BCO" and ext in (".xlsx", ".xls"):
            parser_used = "parse_ec_efect_bco_xlsx"
            df = parse_ec_efect_bco_xlsx(path, DEBUG)
        elif tipo == "INV_BILLETES_ATM":
            if ext == ".pdf":
                parser_used = "parse_inv_billetes_pdf_atm"
                df = parse_inv_billetes_pdf_atm(path, DEBUG)
            elif ext in (".xlsx", ".xls"):
                parser_used = "parse_inv_billetes_xlsx"
                df = parse_inv_billetes_xlsx(path, DEBUG)
        elif tipo == "INV_BILLETES_BCO":
            if ext == ".pdf":
                parser_used = "parse_inv_billetes_pdf_bco"
                df = parse_inv_billetes_pdf_bco(path, DEBUG)
            elif ext in (".xlsx", ".xls"):
                parser_used = "parse_inv_billetes_xlsx"
                df = parse_inv_billetes_xlsx(path, DEBUG)
        else:
            log_warn(
                f"-> No hay un parser definido para tipo='{tipo}' y extensión='{ext}'"
            )

    except Exception as e:
        log_error(
            f"-> ¡ERROR! El parser '{parser_used}' falló para '{path.name}'. Detalles: {e}",
            exc_info=True,
        )
        return pd.DataFrame(), tipo, agencia_final, False

    log_info(f"-> Parser ejecutado: {parser_used}. Registros obtenidos: {len(df)}")

    # Mensaje explícito cuando no se generaron registros (y no era 'SIN MOVIMIENTOS')
    if df.empty:
        log_info(
            f"[SIN REGISTROS] '{path.name}' no generó registros para el tipo {tipo}."
        )

    return df, tipo, agencia_final, True


# -------------------------------------------------------------------
# MOVIMIENTO DE ARCHIVOS A PROCESADO
# -------------------------------------------------------------------

def move_original(path: Path, agencia: str, procesado_ok: bool) -> None:
    """
    Mueve el archivo original a la carpeta:
        PROCESADO/AAAA-MM-DD/AGENCIA/

    - Se deja una copia con el nombre original.
    - El archivo "procesado" se renombra con sufijo _OK o _ERROR + timestamp.
    """
    today_str = datetime.now().strftime("%Y-%m-%d")
    agencia_dir_name = (agencia or "").upper()
    if agencia_dir_name not in AGENCIES:
        agencia_dir_name = "SIN_AGENCIA"

    dest_dir = PROCESADO / today_str / agencia_dir_name
    dest_dir.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    status = "OK" if procesado_ok else "ERROR"
    processed_name = f"{path.stem}_{status}_{timestamp}{path.suffix}"

    original_dest = dest_dir / path.name
    processed_dest = dest_dir / processed_name

    try:
        shutil.copy2(str(path), str(original_dest))
        shutil.move(str(path), str(processed_dest))
        log_info(
            f"-> [MOVIDO] '{path.name}' a '{processed_dest}' "
            f"(copia en '{original_dest.name}')"
        )
    except Exception as e:
        log_error(
            f"-> ¡ERROR! No se pudo mover/copiar el archivo '{path.name}'. Detalles: {e}"
        )


# -------------------------------------------------------------------
# VALIDACIÓN Y SCAN DE PENDIENTES
# -------------------------------------------------------------------

def validate_pending_agencies() -> bool:
    """
    Verifica que cada carpeta de agencia bajo PENDIENTES (ROOT) exista
    y tenga al menos un archivo pendiente (.xlsx/.xls/.pdf).

    Si alguna está ausente o vacía, se loguea error y se aborta el proceso.
    """
    problemas: List[str] = []

    for agencia in AGENCIES:
        carpeta = PENDIENTES / agencia
        if not carpeta.is_dir():
            problemas.append(f"{agencia}: carpeta no existe -> {carpeta}")
            continue

        archivos = [
            p
            for p in carpeta.rglob("*")
            if p.is_file()
            and p.suffix.lower() in (".xlsx", ".xls", ".pdf")
            and not p.name.startswith("~")
        ]
        if not archivos:
            problemas.append(f"{agencia}: carpeta sin archivos pendientes -> {carpeta}")

    if problemas:
        log_error(
            "Hay agencias sin archivos en la carpeta de pendientes. "
            "NO se procesará ningún archivo."
        )
        for p in problemas:
            log_error(f" - {p}")
        return False

    return True


def collect_pending_files() -> List[Tuple[Path, Optional[str]]]:
    """Recopila todos los archivos pendientes en las carpetas de agencias."""
    results: List[Tuple[Path, Optional[str]]] = []
    for agencia in AGENCIES:
        base_dir = PENDIENTES / agencia
        if not base_dir.is_dir():
            continue
        for p in base_dir.rglob("*"):
            if (
                p.is_file()
                and p.suffix.lower() in (".xlsx", ".xls", ".pdf")
                and not p.name.startswith("~")
            ):
                results.append((p, agencia))
    return results


# -------------------------------------------------------------------
# MAIN
# -------------------------------------------------------------------

def run(DEBUG: bool = False) -> Dict[str, int]:
    """Función principal que orquesta todo el proceso."""
    setup_logger()  # inicializa logger y crea CONSOLIDADO/AAAA-MM-DD

    # Validar que todas las agencias tengan archivos pendientes
    if not validate_pending_agencies():
        stats = {k: 0 for k in OUTPUT_FILES.keys()}
        log_info(
            "[ABORTADO] Validación de carpetas de pendientes falló; "
            "no se procesó ningún archivo."
        )
        log_info("=" * 10 + " FIN DE EJECUCIÓN " + "=" * 10 + "\n")
        return stats

    pendientes = collect_pending_files()
    log_info(f"Archivos encontrados para procesar: {len(pendientes)}")

    all_dataframes: Dict[str, List[pd.DataFrame]] = {k: [] for k in OUTPUT_FILES}

    for path, agencia_hint in pendientes:
        df, tipo, agencia_final, procesado_ok_flag = process_file(
            path, parent_agency_hint=agencia_hint, DEBUG=DEBUG
        )

        # Procesado exitoso a efectos de mover
        procesado_exitoso = procesado_ok_flag and (
            not df.empty or (tipo is not None and detect_sin_movimientos(path))
        )

        # Solo acumulamos si hubo datos y el archivo era válido de Itaú
        if procesado_ok_flag and tipo and tipo in all_dataframes and not df.empty:
            all_dataframes[tipo].append(df)

        # Mover a PROCESADO con sufijo OK/ERROR
        move_original(path, agencia_final or agencia_hint or "", procesado_exitoso)
        log_info("-" * 50)

    # Consolidar y escribir resultados finales
    final_consolidated: Dict[str, pd.DataFrame] = {}
    for tipo, df_list in all_dataframes.items():
        if df_list:
            final_consolidated[tipo] = pd.concat(df_list, ignore_index=True)

    final_formatted = post_process_dataframes(final_consolidated)
    write_all_consolidated(final_formatted)

    # Estadísticas finales
    stats = {k: 0 for k in OUTPUT_FILES.keys()}
    for tipo, df in final_formatted.items():
        stats[tipo] = len(df)

    resumen_partes: List[str] = []
    for k, v in stats.items():
        resumen_partes.append(f"{k}: {v}")
    resumen_texto = ", ".join(resumen_partes)
    log_info(f"[RESUMEN FINAL] Registros añadidos: {resumen_texto}")
    log_info("=" * 10 + " FIN DE EJECUCIÓN " + "=" * 10 + "\n")
    return stats


if __name__ == "__main__":
    DEBUG = "DEBUG" in sys.argv
    run(DEBUG=DEBUG)
